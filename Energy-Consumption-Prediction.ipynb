{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Brief: Fundamentals of Numpy and Pandas for Machine Learning  \n",
    "\n",
    "## Deadline: 01 November 2024, 14:00 GMT\n",
    "\n",
    "## Number of marks available: 10\n",
    "\n",
    "In this practical, we will practice using numpy and pandas to implement the fundamentals of machine learning experiments such as data splitting and model training and evaluation. \n",
    "\n",
    "### Please READ the whole assignment first, before starting to work on it.\n",
    "\n",
    "### How and what to submit\n",
    "\n",
    "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed.\n",
    "\n",
    "B. Name your Notebook **COM61011_AssignmentA1_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de. Example: `COM61011_AssignmentA1_abc18de.ipynb`\n",
    "\n",
    "C. Upload the Jupyter Notebook in B to Blackboard under the **Group A: Computing Assignment 1** submission area before the deadline. **There are two submissions: please pay close attention to submit to the right place!**\n",
    "\n",
    "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already. \n",
    "\n",
    "\n",
    "### Assessment Criteria \n",
    "\n",
    "* Being able to use numpy and pandas to preprocess a dataset.\n",
    "\n",
    "* Being able to follow the steps involved in an end-to-end project in machine learning.\n",
    "\n",
    "* Be able to implement, from scratch, a linear model and train it using gradient descent.\n",
    "\n",
    "\n",
    "### Code quality and use of Python libraries\n",
    "When writing your code, you will find out that there are operations that are repeated at least twice. If your code is unreadable, we may not award marks for that section. Make sure to check the following:\n",
    "\n",
    "* Did you include Python functions to solve the question and avoid repeating code? \n",
    "* Did you comment your code to make it readable to others?\n",
    "\n",
    "**DO NOT USE scikit-learn for the questions on this assignment. You are meant to write Python code from scratch. Using scikit-learn for the questions on this assignment will give ZERO marks. No excuse will be accepted.**\n",
    "\n",
    "Furthermore, please try to avoid using any imports apart from the ones already provided in the Notebook. You can easily install all recommended modules for this assignment by running the following command in your terminal: `python -m pip install -r requirements.txt`\n",
    "\n",
    "\n",
    "### Late submissions\n",
    "\n",
    "We follow Department's guidelines about late submissions, i.e., a deduction of 10% of the mark each 24 hours the work is late after the deadline. NO late submission will be marked one week after the deadline. Please read [this link](https://wiki.cs.manchester.ac.uk/index.php/UGHandbook23:Main#Late_Submission_of_Coursework_Penalty). \n",
    "\n",
    "### Use of unfair means \n",
    "\n",
    "**Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.** Please carefully read [what constitutes Unfair Means](https://documents.manchester.ac.uk/display.aspx?DocID=2870) if not sure. If you still have questions, please ask your Personal tutor or the Lecturers.\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: regularised ridge regression and gradient descent\n",
    "\n",
    "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include [Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics)), [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) and the [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization). \n",
    "\n",
    "Here we will build a Ridge Regression model, and implement equations to optimise the objective function using the update rules for gradient descent. You will use those update rules for making predictions on an energy use dataset.\n",
    "\n",
    "### Ridge Regression\n",
    "\n",
    "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_N]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 3, this is, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{N,1} & \\cdots & x_{N, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_N^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objective function** we are going to use has the following form\n",
    "\n",
    "$$ E(\\mathbf{w}, \\lambda) = \\frac{1}{N}\\sum_{n=1}^N (y_n - f(\\mathbf{x}_n))^2 + \\frac{\\lambda}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\lambda>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "This objective function was studied in Lecture 3. \n",
    "\n",
    "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\lambda$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\lambda$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
    "\n",
    "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
    "\n",
    "### Optimising the objective function with respect to $\\mathbf{w}$\n",
    "\n",
    "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}},\n",
    "$$\n",
    "where $\\eta$ is the *learning rate* parameter and $\\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
    "\n",
    "It can be shown (this is a question in the Exercise Sheet 3) that a closed-form expression for the optimal $\\mathbf{w}_*$ is given as\n",
    "\n",
    "\\begin{align*}            \n",
    "            \\mathbf{w}_*& = \\left(\\mathbf{X}^{\\top}\\mathbf{X} + \\frac{\\lambda N}   \n",
    "                                     {2}\\mathbf{I}\\right)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}.\n",
    "\\end{align*}\n",
    "\n",
    "Alternatively, we can find an update equation for $\\mathbf{w}_{\\text{new}}$ using gradient descent leading to:\n",
    "\n",
    "\\begin{align*}\n",
    "   \\mathbf{w}_{\\text{new}} & = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}\n",
    "                              {d\\mathbf{w}},\\\\\n",
    "                           & = \\mathbf{w}_{\\text{old}} +  \\frac{2\\eta}{N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n  \n",
    "                       - \\eta\\lambda\\mathbf{w}_{\\text{old}}\\\\\n",
    "                           & = (1 - \\eta\\lambda)\\mathbf{w}_{\\text{old}} + \\frac{2\\eta}\n",
    "                               {N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-set up: imports and random seed\n",
    "\n",
    "**Important: set a random seed below that corresponds to the last five digits of your student ID number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request, os, zipfile\n",
    "\n",
    "rng = np.random.default_rng(79951) # replace xxxxx with the last 5 digits of your student ID 生成随机数变量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset that we will be using is from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), a popular repository for open source datasets for educational and research purposes. We are going to use ridge regression to predict the energy use of appliances in a low energy building. The dataset is available [here](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction) with an [accompanying paper](https://www.sciencedirect.com/science/article/pii/S0378778816308970?via%3Dihub).\n",
    "\n",
    "We can view some of the rows in the dataset with the `.sample()` method, or print the first few rows with the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "if not os.path.exists('./energydata_complete.csv'):\n",
    "    durl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv\"\n",
    "    save_path = \"./energydata_complete.csv\"\n",
    "    urllib.request.urlretrieve(durl, save_path)\n",
    "\n",
    "# # Read the data into a pandas dataframe\n",
    "energy_appliances_full = pd.read_csv('./energydata_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>...</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>rv1</th>\n",
       "      <th>rv2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-11 17:00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>47.596667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.033333</td>\n",
       "      <td>45.53</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>733.5</td>\n",
       "      <td>92.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.275433</td>\n",
       "      <td>13.275433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-11 17:10:00</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.693333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.722500</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.790000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.066667</td>\n",
       "      <td>45.56</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>733.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>5.2</td>\n",
       "      <td>18.606195</td>\n",
       "      <td>18.606195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-11 17:20:00</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.300000</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.626667</td>\n",
       "      <td>19.79</td>\n",
       "      <td>44.933333</td>\n",
       "      <td>18.926667</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.50</td>\n",
       "      <td>6.366667</td>\n",
       "      <td>733.7</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>55.333333</td>\n",
       "      <td>5.1</td>\n",
       "      <td>28.642668</td>\n",
       "      <td>28.642668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-11 17:30:00</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.590000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>733.8</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.410389</td>\n",
       "      <td>45.410389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-11 17:40:00</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>19.89</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>19.2</td>\n",
       "      <td>44.530000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>45.40</td>\n",
       "      <td>6.133333</td>\n",
       "      <td>733.9</td>\n",
       "      <td>92.0</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>47.666667</td>\n",
       "      <td>4.9</td>\n",
       "      <td>10.084097</td>\n",
       "      <td>10.084097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date  Appliances  lights     T1       RH_1    T2       RH_2  \\\n",
       "0  2016-01-11 17:00:00          60      30  19.89  47.596667  19.2  44.790000   \n",
       "1  2016-01-11 17:10:00          60      30  19.89  46.693333  19.2  44.722500   \n",
       "2  2016-01-11 17:20:00          50      30  19.89  46.300000  19.2  44.626667   \n",
       "3  2016-01-11 17:30:00          50      40  19.89  46.066667  19.2  44.590000   \n",
       "4  2016-01-11 17:40:00          60      40  19.89  46.333333  19.2  44.530000   \n",
       "\n",
       "      T3       RH_3         T4  ...         T9   RH_9     T_out  Press_mm_hg  \\\n",
       "0  19.79  44.730000  19.000000  ...  17.033333  45.53  6.600000        733.5   \n",
       "1  19.79  44.790000  19.000000  ...  17.066667  45.56  6.483333        733.6   \n",
       "2  19.79  44.933333  18.926667  ...  17.000000  45.50  6.366667        733.7   \n",
       "3  19.79  45.000000  18.890000  ...  17.000000  45.40  6.250000        733.8   \n",
       "4  19.79  45.000000  18.890000  ...  17.000000  45.40  6.133333        733.9   \n",
       "\n",
       "   RH_out  Windspeed  Visibility  Tdewpoint        rv1        rv2  \n",
       "0    92.0   7.000000   63.000000        5.3  13.275433  13.275433  \n",
       "1    92.0   6.666667   59.166667        5.2  18.606195  18.606195  \n",
       "2    92.0   6.333333   55.333333        5.1  28.642668  28.642668  \n",
       "3    92.0   6.000000   51.500000        5.0  45.410389  45.410389  \n",
       "4    92.0   5.666667   47.666667        4.9  10.084097  10.084097  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 5 random rows of the data\n",
    "energy_appliances_full.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, the column headings are are printed below:\n",
    "\n",
    "| Data variables | Units |\n",
    "|----------------|-------|\n",
    "Date time stamp year-month-day | hour:min:s\n",
    "| Appliances energy consumption | Wh |\n",
    "| Light energy consumption | Wh | \n",
    "| T1, Temperature in kitchen area | ◦C | \n",
    "| RH1, Humidity in kitchen area | % | \n",
    "| T2, Temperature in living room area | ◦C | \n",
    "| RH2, Humidity in living room area | % | \n",
    "T3, Temperature in laundry room area | ◦C \n",
    "RH3, Humidity in laundry room area | % \n",
    "T4, Temperature in office room | ◦C \n",
    "RH4, Humidity in office room | % \n",
    "T5, Temperature in bathroom | ◦C \n",
    "RH5, Humidity in bathroom | % \n",
    "T6, Temperature outside the building (north side) | ◦C \n",
    "RH6, Humidity outside the building (north side) | % \n",
    "T7, Temperature in ironing room | ◦C \n",
    "RH7, Humidity in ironing room | % \n",
    "T8, Temperature in teenager room 2 | ◦C \n",
    "RH8, Humidity in teenager room 2 | % \n",
    "T9, Temperature in parents room | ◦C \n",
    "RH9, Humidity in parents room | % \n",
    "To, Temperature outside (from Chièvres weather station) | ◦C \n",
    "Pressure (from Chièvres weather station) | mm Hg \n",
    "RHo, Humidity outside (from Chièvres weather station) | % \n",
    "Windspeed (from Chièvres weather station) | m/s \n",
    "Visibility (from Chièvres weather station) | km \n",
    "Tdewpoint (from Chièvres weather station) | ◦C \n",
    "Random Variable 1 (RV 1) | Non dimensional\n",
    "Random Variable 2 (RV 2) | Non dimensional "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to first perform some minor data cleaning. \n",
    "\n",
    "We can't use `datetime` directly, since it's not a number, and encoding it as a continuous variable also creates issues. So let's try to extract some information from it that we can use. In this case, we'll use a binary variable to encode whether the day is a weekday or weekend. These are called categorical or dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Appliances</th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>...</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>is_weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4870</th>\n",
       "      <td>120</td>\n",
       "      <td>10</td>\n",
       "      <td>20.323333</td>\n",
       "      <td>41.260000</td>\n",
       "      <td>18.890000</td>\n",
       "      <td>41.400000</td>\n",
       "      <td>20.50</td>\n",
       "      <td>39.826667</td>\n",
       "      <td>19.70</td>\n",
       "      <td>38.466667</td>\n",
       "      <td>...</td>\n",
       "      <td>42.857222</td>\n",
       "      <td>18.00</td>\n",
       "      <td>39.900000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>737.700000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>59.666667</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11871</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>20.823333</td>\n",
       "      <td>42.730000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>21.60</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>19.60</td>\n",
       "      <td>41.933333</td>\n",
       "      <td>...</td>\n",
       "      <td>43.930000</td>\n",
       "      <td>19.50</td>\n",
       "      <td>42.090000</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>752.100000</td>\n",
       "      <td>90.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>63.500000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>43.966667</td>\n",
       "      <td>23.39</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>19.79</td>\n",
       "      <td>40.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>48.790000</td>\n",
       "      <td>20.00</td>\n",
       "      <td>43.700000</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>97.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>51.333333</td>\n",
       "      <td>6.433333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>21.633333</td>\n",
       "      <td>46.590000</td>\n",
       "      <td>21.166667</td>\n",
       "      <td>44.433333</td>\n",
       "      <td>22.10</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>20.23</td>\n",
       "      <td>46.156667</td>\n",
       "      <td>...</td>\n",
       "      <td>49.635000</td>\n",
       "      <td>18.29</td>\n",
       "      <td>48.126667</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>756.900000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9397</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>22.323333</td>\n",
       "      <td>35.466667</td>\n",
       "      <td>19.566667</td>\n",
       "      <td>37.126667</td>\n",
       "      <td>22.79</td>\n",
       "      <td>35.700000</td>\n",
       "      <td>21.29</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>37.656667</td>\n",
       "      <td>19.60</td>\n",
       "      <td>37.360000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>765.716667</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>-1.816667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Appliances  lights         T1       RH_1         T2       RH_2     T3  \\\n",
       "4870          120      10  20.323333  41.260000  18.890000  41.400000  20.50   \n",
       "11871          50       0  20.823333  42.730000  19.000000  45.500000  21.60   \n",
       "10874          50       0  22.700000  39.500000  19.666667  43.966667  23.39   \n",
       "3174          100      30  21.633333  46.590000  21.166667  44.433333  22.10   \n",
       "9397           40       0  22.323333  35.466667  19.566667  37.126667  22.79   \n",
       "\n",
       "            RH_3     T4       RH_4  ...       RH_8     T9       RH_9  \\\n",
       "4870   39.826667  19.70  38.466667  ...  42.857222  18.00  39.900000   \n",
       "11871  39.500000  19.60  41.933333  ...  43.930000  19.50  42.090000   \n",
       "10874  39.500000  19.79  40.900000  ...  48.790000  20.00  43.700000   \n",
       "3174   46.000000  20.23  46.156667  ...  49.635000  18.29  48.126667   \n",
       "9397   35.700000  21.29  32.200000  ...  37.656667  19.60  37.360000   \n",
       "\n",
       "           T_out  Press_mm_hg     RH_out  Windspeed  Visibility  Tdewpoint  \\\n",
       "4870    3.200000   737.700000  91.000000   2.666667   59.666667   1.900000   \n",
       "11871  12.200000   752.100000  90.500000   6.000000   63.500000  10.700000   \n",
       "10874   6.833333   749.000000  97.333333   3.666667   51.333333   6.433333   \n",
       "3174    7.300000   756.900000  90.000000   5.000000   40.000000   5.800000   \n",
       "9397    3.333333   765.716667  69.666667   3.833333   21.666667  -1.816667   \n",
       "\n",
       "       is_weekday  \n",
       "4870            0  \n",
       "11871           0  \n",
       "10874           0  \n",
       "3174            1  \n",
       "9397            1  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the datetime from the date column and save in a separate dataframe. We'll have to treat this specially in order\n",
    "# to use it in regression. 转化时间类型方便后续提取\n",
    "datetime = pd.to_datetime(energy_appliances_full['date']) \n",
    "\n",
    "# Drop the date and last two columns (rv1 and rv2) as they are not useful for regression. 删除3列\n",
    "energy_appliances = energy_appliances_full.drop(['date', 'rv1', 'rv2'], axis=1)\n",
    "\n",
    "# Create a new column with a binary dummy variable: 1 if the day is a weekday, 0 if it is a weekend 创建新列\n",
    "energy_appliances['is_weekday'] = (datetime.dt.dayofweek < 5).astype(int)\n",
    "\n",
    "energy_appliances.sample(5) #随机抽样"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset\n",
    "\n",
    "Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. *It is important to remember that the test data has to be set aside before preprocessing*. \n",
    "\n",
    "Any preprocessing that you do has to be calibrated *only* on the training data, and several key statistics from this preprocessing need to be saved for the test stage. Separating the dataset into training and test before any preprocessing has happened helps us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "Furthermore, we are going to use *hold-out validation* for validating our predictive model, so we need to further separate the training data into a training set and a validation set.\n",
    "\n",
    "In this step, we will first **shuffle the data**, then split the dataset into a training set, a validation set and a test set: \n",
    "- The training set will have 70% of the total observations,\n",
    "- The validation set will have 15% of the total observations,\n",
    "- The test set will have the remaining 15%. \n",
    "\n",
    "If this doesn't run, check you have correctly initialised the `default_rng()` object with a random seed in **Pre-set up** above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = energy_appliances.shape[0] #数据点总数\n",
    "index = np.arange(ndata)  #创建索引 并储存\n",
    "rng.shuffle(index)                        # Permute the indexes 打乱索引\n",
    "Ntrain = np.int64(np.round(0.70*ndata))   # We compute Ntrain, the number of training instances 训练集大小\n",
    "Nval = np.int64(np.round(0.15*ndata))     # We compute Nval, the number of validation instances   验证集大小\n",
    "Ntest = ndata - Ntrain - Nval             # We compute Ntest, the number of test instances。验证集大小\n",
    "\n",
    "# Split the data into training, validation and test sets\n",
    "# We note that the first column (index 0) is the target variable, what we're trying to predict\n",
    "data_training = energy_appliances.iloc[index[0:Ntrain], 1:].copy() # Select the training data\n",
    "labels_training = energy_appliances.iloc[index[0:Ntrain], 0].copy() # Select the training labels\n",
    "\n",
    "data_val = energy_appliances.iloc[index[Ntrain:Ntrain+Nval], 1:].copy() # Select the validation data\n",
    "labels_val = energy_appliances.iloc[index[Ntrain:Ntrain+Nval], 0].copy() # Select the validation labels\n",
    "\n",
    "data_test = energy_appliances.iloc[index[Ntrain+Nval:ndata], 1:].copy() # Select the test data\n",
    "labels_test = energy_appliances.iloc[index[Ntrain+Nval:ndata], 0].copy() # Select the test labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "It's important to preprocess the data before fitting a model. This includes:\n",
    "- Handling missing values\n",
    "- Scale the data\n",
    "- Encoding categorical or time variables\n",
    "\n",
    "We have already completed the encoding of the datetime variable above, and there are no missing values in this dataset. The only thing left to do is scale the data. Since most of our data is normally distributed, we will use a process called standardization, which scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Note that we *don't* standardize our categorical variable (`is_weekday`), since it's not a continuous variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lights</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH_1</th>\n",
       "      <th>T2</th>\n",
       "      <th>RH_2</th>\n",
       "      <th>T3</th>\n",
       "      <th>RH_3</th>\n",
       "      <th>T4</th>\n",
       "      <th>RH_4</th>\n",
       "      <th>T5</th>\n",
       "      <th>...</th>\n",
       "      <th>RH_8</th>\n",
       "      <th>T9</th>\n",
       "      <th>RH_9</th>\n",
       "      <th>T_out</th>\n",
       "      <th>Press_mm_hg</th>\n",
       "      <th>RH_out</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Visibility</th>\n",
       "      <th>Tdewpoint</th>\n",
       "      <th>is_weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>1.381400e+04</td>\n",
       "      <td>13814.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.774557e-17</td>\n",
       "      <td>2.225654e-15</td>\n",
       "      <td>1.806962e-15</td>\n",
       "      <td>-5.121782e-16</td>\n",
       "      <td>4.269223e-17</td>\n",
       "      <td>-5.966625e-16</td>\n",
       "      <td>-6.689307e-16</td>\n",
       "      <td>4.012041e-17</td>\n",
       "      <td>-5.791741e-16</td>\n",
       "      <td>2.980741e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>7.520005e-16</td>\n",
       "      <td>1.586814e-15</td>\n",
       "      <td>1.862513e-15</td>\n",
       "      <td>-3.497677e-17</td>\n",
       "      <td>-5.289722e-15</td>\n",
       "      <td>-2.844434e-16</td>\n",
       "      <td>4.937897e-17</td>\n",
       "      <td>1.113599e-16</td>\n",
       "      <td>-5.606570e-17</td>\n",
       "      <td>0.718474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.449760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.801864e-01</td>\n",
       "      <td>-3.045341e+00</td>\n",
       "      <td>-3.330896e+00</td>\n",
       "      <td>-1.934538e+00</td>\n",
       "      <td>-4.894466e+00</td>\n",
       "      <td>-2.522802e+00</td>\n",
       "      <td>-3.192677e+00</td>\n",
       "      <td>-2.813160e+00</td>\n",
       "      <td>-2.374830e+00</td>\n",
       "      <td>-2.310059e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.543059e+00</td>\n",
       "      <td>-2.280845e+00</td>\n",
       "      <td>-2.966052e+00</td>\n",
       "      <td>-2.326881e+00</td>\n",
       "      <td>-3.541319e+00</td>\n",
       "      <td>-3.698849e+00</td>\n",
       "      <td>-1.647876e+00</td>\n",
       "      <td>-3.152170e+00</td>\n",
       "      <td>-2.476482e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-4.801864e-01</td>\n",
       "      <td>-5.900917e-01</td>\n",
       "      <td>-7.297961e-01</td>\n",
       "      <td>-6.916682e-01</td>\n",
       "      <td>-6.203877e-01</td>\n",
       "      <td>-7.285426e-01</td>\n",
       "      <td>-7.151844e-01</td>\n",
       "      <td>-6.570573e-01</td>\n",
       "      <td>-8.024090e-01</td>\n",
       "      <td>-7.167975e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.406907e-01</td>\n",
       "      <td>-7.312590e-01</td>\n",
       "      <td>-7.233437e-01</td>\n",
       "      <td>-7.097622e-01</td>\n",
       "      <td>-6.151758e-01</td>\n",
       "      <td>-6.347148e-01</td>\n",
       "      <td>-8.288350e-01</td>\n",
       "      <td>-7.898572e-01</td>\n",
       "      <td>-6.866497e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-4.801864e-01</td>\n",
       "      <td>-4.794269e-02</td>\n",
       "      <td>-1.491559e-01</td>\n",
       "      <td>-1.492000e-01</td>\n",
       "      <td>1.908636e-02</td>\n",
       "      <td>-7.381268e-02</td>\n",
       "      <td>-2.129067e-01</td>\n",
       "      <td>-1.180318e-01</td>\n",
       "      <td>-1.390248e-01</td>\n",
       "      <td>-1.023312e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.176028e-01</td>\n",
       "      <td>-3.867869e-02</td>\n",
       "      <td>-1.672005e-01</td>\n",
       "      <td>-9.157496e-02</td>\n",
       "      <td>7.571899e-02</td>\n",
       "      <td>2.789544e-01</td>\n",
       "      <td>-1.463009e-01</td>\n",
       "      <td>1.381941e-01</td>\n",
       "      <td>-7.938518e-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-4.801864e-01</td>\n",
       "      <td>5.752171e-01</td>\n",
       "      <td>7.024496e-01</td>\n",
       "      <td>5.374686e-01</td>\n",
       "      <td>6.978623e-01</td>\n",
       "      <td>5.209420e-01</td>\n",
       "      <td>7.639154e-01</td>\n",
       "      <td>6.170031e-01</td>\n",
       "      <td>7.129084e-01</td>\n",
       "      <td>5.556370e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>6.817507e-01</td>\n",
       "      <td>5.642150e-01</td>\n",
       "      <td>6.679368e-01</td>\n",
       "      <td>5.579923e-01</td>\n",
       "      <td>7.304886e-01</td>\n",
       "      <td>7.915005e-01</td>\n",
       "      <td>6.044867e-01</td>\n",
       "      <td>1.381941e-01</td>\n",
       "      <td>6.667112e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.360839e+00</td>\n",
       "      <td>2.855982e+00</td>\n",
       "      <td>5.842377e+00</td>\n",
       "      <td>4.362975e+00</td>\n",
       "      <td>3.531691e+00</td>\n",
       "      <td>3.492716e+00</td>\n",
       "      <td>3.371856e+00</td>\n",
       "      <td>2.626098e+00</td>\n",
       "      <td>2.784637e+00</td>\n",
       "      <td>3.328892e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.034152e+00</td>\n",
       "      <td>2.507426e+00</td>\n",
       "      <td>2.839359e+00</td>\n",
       "      <td>3.526546e+00</td>\n",
       "      <td>2.270326e+00</td>\n",
       "      <td>1.348616e+00</td>\n",
       "      <td>3.880651e+00</td>\n",
       "      <td>2.331770e+00</td>\n",
       "      <td>2.797131e+00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lights            T1          RH_1            T2          RH_2  \\\n",
       "count  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean  -1.774557e-17  2.225654e-15  1.806962e-15 -5.121782e-16  4.269223e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -4.801864e-01 -3.045341e+00 -3.330896e+00 -1.934538e+00 -4.894466e+00   \n",
       "25%   -4.801864e-01 -5.900917e-01 -7.297961e-01 -6.916682e-01 -6.203877e-01   \n",
       "50%   -4.801864e-01 -4.794269e-02 -1.491559e-01 -1.492000e-01  1.908636e-02   \n",
       "75%   -4.801864e-01  5.752171e-01  7.024496e-01  5.374686e-01  6.978623e-01   \n",
       "max    8.360839e+00  2.855982e+00  5.842377e+00  4.362975e+00  3.531691e+00   \n",
       "\n",
       "                 T3          RH_3            T4          RH_4            T5  \\\n",
       "count  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean  -5.966625e-16 -6.689307e-16  4.012041e-17 -5.791741e-16  2.980741e-16   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -2.522802e+00 -3.192677e+00 -2.813160e+00 -2.374830e+00 -2.310059e+00   \n",
       "25%   -7.285426e-01 -7.151844e-01 -6.570573e-01 -8.024090e-01 -7.167975e-01   \n",
       "50%   -7.381268e-02 -2.129067e-01 -1.180318e-01 -1.390248e-01 -1.023312e-01   \n",
       "75%    5.209420e-01  7.639154e-01  6.170031e-01  7.129084e-01  5.556370e-01   \n",
       "max    3.492716e+00  3.371856e+00  2.626098e+00  2.784637e+00  3.328892e+00   \n",
       "\n",
       "       ...          RH_8            T9          RH_9         T_out  \\\n",
       "count  ...  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean   ...  7.520005e-16  1.586814e-15  1.862513e-15 -3.497677e-17   \n",
       "std    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min    ... -2.543059e+00 -2.280845e+00 -2.966052e+00 -2.326881e+00   \n",
       "25%    ... -7.406907e-01 -7.312590e-01 -7.233437e-01 -7.097622e-01   \n",
       "50%    ... -1.176028e-01 -3.867869e-02 -1.672005e-01 -9.157496e-02   \n",
       "75%    ...  6.817507e-01  5.642150e-01  6.679368e-01  5.579923e-01   \n",
       "max    ...  3.034152e+00  2.507426e+00  2.839359e+00  3.526546e+00   \n",
       "\n",
       "        Press_mm_hg        RH_out     Windspeed    Visibility     Tdewpoint  \\\n",
       "count  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04  1.381400e+04   \n",
       "mean  -5.289722e-15 -2.844434e-16  4.937897e-17  1.113599e-16 -5.606570e-17   \n",
       "std    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "min   -3.541319e+00 -3.698849e+00 -1.647876e+00 -3.152170e+00 -2.476482e+00   \n",
       "25%   -6.151758e-01 -6.347148e-01 -8.288350e-01 -7.898572e-01 -6.866497e-01   \n",
       "50%    7.571899e-02  2.789544e-01 -1.463009e-01  1.381941e-01 -7.938518e-02   \n",
       "75%    7.304886e-01  7.915005e-01  6.044867e-01  1.381941e-01  6.667112e-01   \n",
       "max    2.270326e+00  1.348616e+00  3.880651e+00  2.331770e+00  2.797131e+00   \n",
       "\n",
       "         is_weekday  \n",
       "count  13814.000000  \n",
       "mean       0.718474  \n",
       "std        0.449760  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the data to zero mean and unit variance. Note we do NOT apply this to the labels OR to our binary variable!\n",
    "training_means = data_training.mean() #训练集均值\n",
    "training_stds = data_training.std() #训练集方差\n",
    "data_training_standardized = (data_training - training_means) / training_stds #标准化操作\n",
    "# Replace last column (is_weekend) with original binary value - we don't want to standardize this.\n",
    "data_training_standardized['is_weekday'] = data_training['is_weekday'] #二进制数不需要标准化，还原回去\n",
    "\n",
    "# Let's use describe again: we should see that the mean is 0 (almost - some numerical overflow here) and the standard deviation \n",
    "# is 1 for each feature.\n",
    "data_training_standardized.describe() #描述性统计信息，包括每列的均值、标准差、最小值、最大值等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the preprocessing steps to the validation set as if it were new data: we use the values from the **training** data to standardize the **validation** data. This is important to ensure that the model generalizes well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val_standardized = (data_val - training_means) / training_stds #对验证集再标准化\n",
    "data_val_standardized['is_weekday'] = data_val['is_weekday'] # don't forget to not standardize this column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training a predictive model\n",
    "\n",
    "We have now split our data into training and validation data and applied relevant preprocessing steps. We are now in a good position to work on developing the prediction model and validating it. We will build a regularised ridge regression model and train it using gradient descent for iterative optimisation. \n",
    "\n",
    "We first organise the dataframes into the vector of targets $\\mathbf{y}$, call it `yTrain`, and the design matrix $\\mathbf{X}$, call it `XTrain`. We will augment `XTrain` with a column of ones: this is the design matrix. We repeat the process for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target vector and design matrix for training data 训练集\n",
    "yTrain = np.reshape(labels_training.values, (Ntrain,1)) # The training target labels as a column vector 重塑列向量\n",
    "XTrain = np.concatenate((np.ones((Ntrain,1)), data_training_standardized.values), axis=1) # 创建矩阵，并创建列向量The standardised inputs with an additional column vector  \n",
    "\n",
    "# Do the same for val data 验证集同理\n",
    "yVal = np.reshape(labels_val.values, (Nval,1)) # The validation target labels as a column vector\n",
    "XVal = np.concatenate((np.ones((Nval,1)), data_val_standardized.values), axis=1) # The standardised inputs with an additional column vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the optimal $\\mathbf{w}$ with stochastic gradient descent (5 marks)\n",
    "Now, we will use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set in `XTrain` and `yTrain` to compute the gradient, use a subset of $S$ instances in `XTrain` and `yTrain`. This is sometimes called *minibatch gradient descent,* where $S$ is the size of the minibatch. \n",
    "\n",
    "You will need to find the best values for three parameters: $\\eta$, the learning rate, $S$, the number of datapoints in the minibatch, and $\\lambda$, the regularisation parameter. We can do this using a grid search over the validation set. You should complete the following tasks:\n",
    "\n",
    "* **Write the optimisation function:** Write a function, `sgd_optimiser`, that takes as input the training data and targets, the learning rate $\\eta$, the minibatch size $S$, the regularisation parameter $\\lambda$, and the number of iterations $T$. The function should return the optimal $\\mathbf{w}$ after the chosen number of iterations. \n",
    "\n",
    "* **Evaluate each set of hyperparameters:** For each value that you have of $\\lambda$, $\\eta$ and $S$ in your grid, use the training set to compute $\\mathbf{w}$ using your `sgd_optimiser` function, and then measure the RMSE using that $\\mathbf{w}$ over the validation data. For the minibatch gradient descent choose to stop the iterative procedure after $500$ iterations. \n",
    "\n",
    "* Choose the values of $\\lambda$, $\\eta$ and $S$ that lead to the lowest RMSE and save them. You will use them at the test stage.\n",
    "\n",
    "When writing these functions, you should avoid using for loops over individual features or samples; that is, you should be able to calculate the gradient, weight updates, and MSE in vectorised form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the search space by creating a grid of values for the parameters $\\lambda$ and $\\eta$ using `np.logspace` and a grid of values for $S$ using `np.linspace`. Because we need to find three parameters, let's start with five values for each parameter in the grid (see if you can increase it - it may take some time to run). Make sure you understand the meaning of `np.logspace` and `np.linspace`. Notice that you can use negative values for `start` in `np.logspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE GRID OF VALUES FOR LAMBDA, ETA AND S\n",
    "num = 5\n",
    "lambda_vector = np.logspace(-4, -1, num) #np.logspace函数生成一个对数间隔的向量作为lambda的候选值 10的-4次\n",
    "eta_vector = np.logspace(-5, -2, num)\n",
    "S_vector = np.linspace(10, 200, num, dtype=int) # we want integer values for S (n_samples) np.linspace函数生成的是在线性尺度上等间隔的值。这里的范围是从10到200，并且生成num个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lambda: 0.0001\n",
      "best eta: 0.01\n",
      "best S: 152\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE REGULARISED LINEAR MODEL AND COMPUTE THE RMSE FOR ALL VALUES OF LAMBDA, ETA AND S\n",
    "# note that 'lambda' is a reserved keyword in Python, so we use 'lmbd' instead\n",
    "\n",
    "def sgd_optimiser(X, y, gamma, eta, S, max_iters=500):\n",
    "    # YOUR CODE HERE\n",
    "    w = np.zeros((X.shape[1], 1))  # Initializing the weights 初始化权重\n",
    "    \n",
    "# Start iterating \n",
    "    for i in range(max_iters):\n",
    "        # S samples are randomly drawn from the training set 从训练集中随机抽取S个样本\n",
    "        indices = np.random.choice(X.shape[0], S, replace=False)  # Randomly sampled index 随机抽样索引\n",
    "        X_batch = X[indices]  # batch X 小批量X，Feature matrix of the training data 训练数据的特征矩阵\n",
    "        y_batch = y[indices]  # batch Y 小批量y Vector of target values for the training data 训练数据的目标值向量\n",
    "\n",
    "        # Calculating predictive values 计算预测值\n",
    "        y_pred = X_batch @ w \n",
    "\n",
    "        # Calculate the gradient based on the provided formula\n",
    "        mse_gradient = -(2 / S) * (X_batch.T @ (y_batch - y_pred))  # MSE gradient\n",
    "        reg_gradient = gamma * w  # Regularization gradient\n",
    "\n",
    "        # Combine gradients\n",
    "        gradient = mse_gradient + reg_gradient\n",
    "\n",
    "        # Update weights using the gradient\n",
    "        w -= eta * gradient\n",
    "\n",
    "    return w \n",
    "    \n",
    "# Run the optimiser and store the RMSE for all combinations of lambda, eta and SC 初始化参数用于存储最优 RMSE 以及对应的超参数组合\n",
    "best_rmse = float('inf')\n",
    "best_lambda = None\n",
    "best_eta = None\n",
    "best_S = None\n",
    "\n",
    "# Perform a grid search to find the best combination of hyperparameters 执行网格搜索，找到最佳的超参数组合\n",
    "for lmbd in lambda_vector:\n",
    "    for eta in eta_vector:\n",
    "        for S in S_vector:\n",
    "            # Call the sgd optimiser function to optimize\n",
    "            w_opt = sgd_optimiser(XTrain, yTrain, lmbd, eta, S, max_iters=500)\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            y_pred_val = XVal @ w_opt\n",
    "            rmse_val = np.sqrt(np.mean((yVal - y_pred_val) ** 2))\n",
    "            \n",
    "            # Update the optimal combination of hyperparameters 更新最优参数\n",
    "            if rmse_val < best_rmse:\n",
    "                best_rmse = rmse_val\n",
    "                best_lambda = lmbd\n",
    "                best_eta = eta\n",
    "                best_S = S\n",
    "\n",
    "# Save the optimal values of lambda, eta and S (print them for our reference)\n",
    "lmbd = best_lambda\n",
    "eta = best_eta\n",
    "S = best_S\n",
    "\n",
    "print(f\"best lambda: {lmbd}\")\n",
    "print(f\"best eta: {eta}\")\n",
    "print(f\"best S: {S}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and results reporting (5 marks)\n",
    "\n",
    "We now know the best model, according to the validation data. We will now put together the training data and the validation data and perform the preprocessing as before, this is, impute the missing values and scale the inputs. We will train the model again using the minibatch stochastic gradient descent and finally compute the RMSE over the test data. You should do the following:\n",
    "\n",
    "* **Prepare the data:** In this question we will use the test data. First, combine the original training and validation data and standardize again using the mean and std. from this combined data. Save the values from this preprocessing step. Use the saved values to preprocess the test data, similarly to how we used the values from training data to preprocess the validation data. Before training and inference, don't forget to add the column of ones to the data to create the design matrix.\n",
    "\n",
    "* **Re-train your model** on the full training set, using the optimal values of $\\lambda$, $\\eta$ and $S$.\n",
    "\n",
    "* **Run your model** on the test data using the optimal values of of $\\lambda$, $\\eta$ and $S$, and report the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training and validation data into a single final training set 合并成一个完整的训练集，这样可以利用所有的数据来训练模型，提高模型的泛化能力\n",
    "data_training_full = pd.concat([data_training, data_val]) #训练和验证数据集合并\n",
    "labels_training_full = pd.concat([labels_training, labels_val])\n",
    "\n",
    "# Standardize the new training set 消除不同特征之间的量纲差异，使得模型训练更加稳定和高效。标准化后的数据有助于梯度下降算法更快地收敛\n",
    "data_training_full_standardized = (data_training_full - training_means) / training_stds\n",
    "data_training_full_standardized['is_weekday'] = data_training_full['is_weekday']\n",
    "\n",
    "\n",
    "# Create the new design matrix and target vector for the full training set 创建新的设计矩阵和目标向量\n",
    "yTrain_full = np.reshape(labels_training_full.values, (Ntrain + Nval, 1))\n",
    "XTrain_full = np.concatenate((np.ones((Ntrain + Nval, 1)), data_training_full_standardized.values), axis=1)\n",
    "\n",
    "\n",
    "# Preprocess the test data and create the design matrix and target vector 预处理测试数据并创建设计矩阵和目标向量\n",
    "data_test_standardized = (data_test - training_means) / training_stds\n",
    "data_test_standardized['is_weekday'] = data_test['is_weekday']\n",
    "yTest = np.reshape(labels_test.values, (Ntest, 1))\n",
    "XTest = np.concatenate((np.ones((Ntest, 1)), data_test_standardized.values), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on the test set: 98.70335197524578\n"
     ]
    }
   ],
   "source": [
    "# Train the regularised linear model and compute the RMSE for the values of gamma, eta and S over the test data\n",
    "w_final = sgd_optimiser(XTrain_full, yTrain_full, lmbd, eta, S, max_iters=500)\n",
    "\n",
    "# Calculate RMSE and print\n",
    "y_pred_test = XTest @ w_final\n",
    "rmse_test = np.sqrt(np.mean((yTest - y_pred_test) ** 2))\n",
    "print(f\"RMSE on the test set: {rmse_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
